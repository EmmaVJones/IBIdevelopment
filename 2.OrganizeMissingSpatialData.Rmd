---
title: "Organize Missing Spatial Info"
author: "Emma Jones"
date: "6/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(sf)
library(readxl)
library(leaflet)
library(leafpop)
library(mapview)
library(inlmisc)
library(lubridate)

# Bring in missing data categories
needWQS <- readRDS( 'data/needSpatial/needWQS.RDS') 
needWatershed <- readRDS('data/needSpatial/needWatershed.RDS')
```

## Watersheds Missing

For this section, we will use existing web scraping techniques to use USGS streamStats API to autodelineate watersheds for stations that need additional landcover information. This script is loosely based on the ConserveVA methodology and will rely upon human review (intern) to verify the 1:24k delineations match the intended watershed.

```{r bring in autodelineation functions}
source('methods/StreamStatsAutoDelineation.R')
```
Begin delineating sites.

Break into chunks to ease processing. Using 50 site blocks. Repeat as many times as necessary.

```{r streamstats delineation}
#bring in already delineated sites
doneWatersheds <- readRDS('watersheds.RDS')
donePoints <- readRDS('points.RDS')

needWatershedFilter <- filter(needWatershed, !StationID %in% doneWatersheds$UID)[1:50,]
#needWatershed[1:50,]

basinData <- streamStats_Delineation(state= 'VA',
                                     longitude = needWatershedFilter$Longitude, 
                                     latitude = needWatershedFilter$Latitude, 
                                     UID = needWatershedFilter$StationID)

```
Then we need to organize these files and check they delineated properly.
    
```{r organize spatial results}
# organize into appropriate files, lossy move here- automatically removes sites with no data
watersheds <- basinData$polygon %>%
  reduce(rbind) %>%
  arrange(UID)
points <- basinData$point %>%
  reduce(rbind) %>%
  arrange(UID)

#saveRDS(watersheds, 'watersheds50.RDS')
#saveRDS(points, 'points50.RDS')
```

Run back out to streamstats and get anyone that is missing
    
```{r fix missing sites (rerun as necessary)}
# fix anything that is missing
if(nrow(points) != nrow(watersheds) | nrow(needWatershedFilter) != nrow(watersheds)){
  missing <- unique(
    c(as.character(points$UID[!(points$UID %in% watersheds$UID)]),
      as.character(needWatershedFilter$StationID[!(needWatershedFilter$StationID %in% watersheds$UID)])))
  missingDat <- filter(needWatershedFilter, StationID %in% missing)
  
  #remove missing site from the paired dataset
  points <- filter(points, ! UID %in% missing)
  watersheds <- filter(watersheds, ! UID %in% missing)
  
  dat <- streamStats_Delineation(state= 'VA', 
                                 longitude = missingDat$Longitude, 
                                 latitude = missingDat$Latitude, 
                                 UID = missingDat$StationID)
  
  watersheds_missing <- dat$polygon %>%
    reduce(rbind)
  
  points_missing <- dat$point %>%
    reduce(rbind)
  
  watersheds <- rbind(watersheds, watersheds_missing) %>%
    arrange(UID)
  
  points <- rbind(points, points_missing) %>%
    arrange(UID)
  
  rm(missingDat); rm(dat); rm(watersheds_missing); rm(points_missing)
}

watersheds1 <- rbind(watersheds, doneWatersheds)
points1 <- rbind(points, donePoints)

saveRDS(watersheds1, 'watersheds.RDS')
saveRDS(points1, 'points.RDS')


```


We are going to do manual QA for each watershed. Bring in Probabilistic Monitoring sample frame for stream order information.

```{r visual QA mapping function}
vafrm <- st_read('data/GIS/vafrm_99_05Albers.shp') %>%
  st_transform(4326) # change to WGS84 for web mapping

mapMe <- function(watershedChoice, pointChoice, initialData, probFrame){
  pointChoice <- mutate(pointChoice, StationID = UID) %>%
    left_join(initialData, by = 'StationID')
  
  segments <- suppressWarnings(st_intersection(probFrame, st_buffer(watershedChoice,0))) # buffer by 0 helps with topology issues
  
#  # the kelly special
#  leaflet(watershedChoice) %>%
#    addProviderTiles(providers$OpenStreetMap, group='Open Street Map') %>%
#    addProviderTiles(providers$Stamen.Terrain, group='Topo') %>%
#    addProviderTiles(providers$Esri.WorldImagery,group='Imagery') %>%

  
  CreateWebMap(maps = c("Topo","Imagery","Hydrography"), collapsed = TRUE) %>%
    setView(-78, 37.5, zoom=6) %>%
    addPolygons(data= watershedChoice,  color = 'black', weight = 1,
                  fillColor='blue', fillOpacity = 0.5,stroke=0.1,
                  group="Watershed",
                  popup=leafpop::popupTable(watershedChoice, zcol=c('UID'))) %>%
    addPolylines(data = segments,  color = 'blue', weight =3,
                 group="Strahler Info", label = ~STRAHLER,
                 popup=leafpop::popupTable(segments, zcol=c('STRAHLER'))) %>% hideGroup("Strahler Info") %>%
    addCircleMarkers(data = pointChoice, color='orange', fillColor='black', radius = 5,
                     fillOpacity = 0.5,opacity=0.5,weight = 1,stroke=T, group="Station",
                     label = ~UID,
                     popup=leafpop::popupTable(pointChoice)) %>%
    addLayersControl(baseGroups=c("Topo","Imagery","Hydrography"),
      #baseGroups=c("Topo","Imagery","Hydrography"),
                       overlayGroups = c('Station',"Strahler Info", 'Watershed'),
                       options=layersControlOptions(collapsed=T),
                       position='topleft')
}
```

And double check all watersheds are appropriately delineated.

```{r visual QA}
# Can be reviewed one at a time
mapMe(watersheds[1,], points[1,], dplyr::select(needWatershed, -UID), vafrm)

# or all at once
mapMe(watersheds, points, dplyr::select(needWatershed, -UID), vafrm)

```


Anything look off? If so, filter out those StationID's before they get saved to a shapefile. You will have to go to streamstats and manually choose the appropriate stream segment, then merge back to full basin watersheds shapefile.
    
```{r drop these sites for manual autodelineation}
# identify problem sites
dropMe <- c('1AXJI000.38')

# drop the sites identified
watersheds <- filter(watersheds, ! UID %in% dropMe)
# note, we are not dropping the DEQ lat/lng from the points file because this is the most accurate location information

# organize lat/lngs for problem sites for delineation on streamstats website
dropMe <- tibble(StationID = dropMe) %>%
  left_join(basin, by = 'StationID')
dropMe
```
    
For the station(s) listed above (if any):
1. Go to https://streamstats.usgs.gov/ss/
2. Enter each lat/lng, separated by a comma
3. Choose the appropriate state (VA)
4. Delineate: Use the 1:100k strahler stream order from the Order field above to choose the appropriate streamstats (1:24k) stream segment.
5. Download Basin as a shapefile.
6. Extract the zip file.
7. Rename the 'globalwatershed' file to the name of the StationID
**Pro Tip- select all four files (.dbf, .prj, .shp, .shx), press F2, paste the StationID into one file, press Enter... all renamed **
8. Move the renamed watershed to GIS/delineations/streamStatsFixes/basinName (make sure there is a folder with the appropriate basin name)
![directorystructure](methods/directoryStructure.png)
Now bring back in the manually autodelineated watersheds and combine with the watersheds that passed QA filters.
    
```{r bring in watersheds}
# Where did you save files relative to the working directory?
savedHere <- 'GIS/delineations/streamStatsFixes/Potomac River Basin'

# read in new watersheds after manual QA
shapes <- gsub('.prj','',list.files( savedHere, pattern="*.prj", full.names=F))
filenames <- paste0(savedHere, '/',shapes, '.shp')
  
# read in shapefiles and organize
newSheds <- filenames %>%
  map(st_read) %>%
  map2(shapes,~mutate(.x,UID=.y)) %>% # make a StationID column
  map(~dplyr::select(.,UID)) %>%
  reduce(rbind) 

# Combine with QAed watersheds
watersheds <- rbind(watersheds, newSheds) %>%
  arrange(UID)
  
```
    
Double check everything looks good in the basin.
    
```{r map double check}
mapMe(watersheds, points, dplyr::select(needWatershed, -UID), vafrm)
```

Save QAed watersheds and sites as shapefiles.
    
```{r save shapefiles}
st_write(watersheds, 'data/GIS/delineations/StreamStats_watersheds.shp')
st_write(points, paste0('data/GIS/delineations/StreamStats_points.shp'))

#basinName <- 'Potomac River Basin'
#watersheds <- st_read(paste0('GIS/delineations/', basinName,'_StreamStats_watersheds.shp'))
#points <- st_read(paste0('GIS/delineations/', basinName,'_StreamStats_points.shp'))
```






## Snap stations to WQS

These stations are missing WQS information and need to be spatially joined to the most recent WQS layer (at the time of scripting) in order to access this information. All the stations are biological stations, so we only need to use the riverine methods.

All stations get manually reviewed after snapping.

```{r snap WQS}

source('snappingFunctions/snapWQS.R')

needWQS <- readRDS( 'data/needSpatial/needWQS.RDS') %>% 
      st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
               remove = F, # remove these lat/lon cols from df
               crs = 4326)
             

WQStable <- tibble(StationID = NA, WQS_ID = NA)

riverine <- st_read('C:/HardDriveBackup/GIS/WQS/WQS_layers_05082020.gdb', layer = 'riverine_05082020' , fid_column_name = "OBJECTID") #%>%
  #st_transform(102003) # forcing to albers from start bc such a huge layer
  #st_transform('+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m no_defs')
# 2/3/21 have to manually feed character proj4string bc doesn't recognize albers epsg 


WQStable <- snapAndOrganizeWQS(needWQS, 'StationID', riverine, 
                               bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                               WQStable)


rm(riverine)
saveRDS(WQStable, 'data/WQStable.RDS')
WQStable <- readRDS('data/WQStable.RDS')
# grab spatial info for sites and save as shapefile for manual WQS review in ArcGIS
library(pins)
# get ODS and server configuration settings
conn <- config::get("connectionSettings")

# connect to server
board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))

WQM_Stations_Spatial <- pin_get('ejones/WQM-Stations-Spatial', board = 'rsconnect')

WQStable <- WQStable %>% 
  mutate(Comments = 'IBI project review') %>% 
  group_by(StationID) %>% 
  mutate(n = n()) 

WQStableForReview <- WQStable %>% 
  filter(n>1) %>% 
  left_join( WQM_Stations_Spatial, by = 'StationID') %>% 
  filter(!is.na(StationID)) %>% 
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
               remove = F, # remove these lat/lon cols from df
               crs = 4326)
#st_write(WQStableForReview, 'data/needSpatial/WQStableForReview.shp')

# after going through individually, bring results back in
WQStableReviewed <- st_read('data/needSpatial/WQStableForReview.shp') %>% 
  dplyr::select('StationID' =  "StatnID", "WQS_ID" , `Buffer Distance` = "BffrDst", `Comments` = "Commnts" )

# smash back together
WQStableFinal <- filter(WQStable,! StationID %in% WQStableForReview$StationID) %>% 
  bind_rows(WQStableReviewed %>% st_drop_geometry()) %>% 
  filter(!is.na(StationID)) %>% 
  dplyr::select(-n)

saveRDS(WQStableFinal, "data/needSpatial/finalWQS.RDS")  
```



## Snap stations to strahler

These stations are missing strahler order information and need to be spatially joined to the 1:100k va probmon sample frame in order to access this information. All the stations are biological stations, so we only need to use the riverine methods.

All stations get manually reviewed after snapping.

```{r snap order}

source('snappingFunctions/snapAnything.R')

needOrder <- readRDS( 'data/needSpatial/needWatershed.RDS') %>% ungroup() %>% 
      st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
               remove = F, # remove these lat/lon cols from df
               crs = 4326)
             

orderTable <- tibble(StationID = NA, STRAHLER = NA) # make sure the name of the second variable matches the name of the field you want to extract from the line network

vafrm <- st_read('data/GIS/vafrm_99_05Albers.shp') #%>%
  #st_transform(102003) # forcing to albers from start bc such a huge layer
  #st_transform('+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m no_defs')
# 2/3/21 have to manually feed character proj4string bc doesn't recognize albers epsg 


orderTable <- snapAndOrganizeAnything(needOrder, 'StationID', vafrm, 'STRAHLER',
                                      bufferDistances = seq(20,80,by=20),  # buffering by 20m from 20 - 80 meters
                                      orderTable)


rm(vafrm)
saveRDS(orderTable, 'data/orderTable.RDS')
```

Now organize and make decisions

```{r go through order}

needWork <- filter(orderTable, is.na(STRAHLER)) # 20 need real fixes

 
orderTableCounts <-  orderTable %>% 
  filter(! is.na(STRAHLER)) %>% 
  group_by(StationID) %>% 
  mutate(n = n())
orderTableMoreThan1 <-  filter(orderTableCounts, n>1)

orderTableDistinct <- orderTableMoreThan1 %>% 
  distinct(STRAHLER, .keep_all = T) %>% 
  mutate(n = n())

final <- bind_rows(filter(orderTableCounts, n == 1),
                   filter(orderTableDistinct, n == 1))

orderTableRealMoreThan1 <- filter(orderTableDistinct, n > 1)

manualReview <- bind_rows(orderTableRealMoreThan1, needWork) %>% ungroup() %>% 
  left_join(needOrder %>%  dplyr::select(StationID:ASSESS_REG) %>% st_drop_geometry(), by = 'StationID') %>% 
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer using these columns
               remove = F, # remove these lat/lon cols from df
               crs = 4326)
     

#st_write(manualReview, 'data/OrderReview.shp')
# now go through and manually fix in arcmap quickly
manualReview <- st_read('data/OrderReview.shp') %>% 
  st_drop_geometry() %>% 
  dplyr::select(StationID  = StatnID, STRAHLER = STRAHLE, `Buffer Distance` = BffrDst)

final <- bind_rows(final, manualReview)

saveRDS(final, 'data/needSpatial/finalOrder.RDS')
```

