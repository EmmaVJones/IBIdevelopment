---
title: "Initial Data Organization"
author: "Emma Jones"
date: "6/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(sf)
library(DBI)
library(lubridate)
library(pool)
library(pins)
library(config)
library(readxl)
library(dbplyr)

# get ODS and server configuration settings
conn <- config::get("connectionSettings")

# connect to server
board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))



# Connect to ODS production environment
# pool <- dbPool(
#   drv = odbc::odbc(),
#   Driver = "ODBC Driver 11 for SQL Server",#Driver = "SQL Server Native Client 11.0",
#   Server= "DEQ-SQLODS-PROD,50000",
#   dbname = "ODS",
#   trusted_connection = "yes")

```

## Bring in Stations with Reference/Stress qualifier

These spreadsheets are organized by DEQ region (one spreadsheet per regional office) where the regional biologists rated potential sites as reference or stressed. Jason Hill QAed the dataset and passed stations along to Emma Jones to query data for Lucy Baker to run NMDS and further exploratory analyses.

As more regional station lists become available, this chunk will fill out with all stations statewide.

```{r regional ref stress station lists}
SCRO <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_SCRO.xlsx', sheet = 'JRH_Final_SCRO') %>%
  mutate(Coastal = FALSE)
SWRO <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_SWRO.xlsx', sheet = 'JRH_Final_SWRO') %>%
  mutate(Coastal = FALSE)
VRO <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_VRO.xlsx', sheet = 'JRH_Final_VRO') %>%
  mutate(Coastal = FALSE)
WCRO <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_WCRO.xlsx', sheet = 'JRH_Final_WCRO') %>%
  mutate(Coastal = FALSE)
NRO <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_NRO.xlsx', sheet = 'JRH_Final_NRO') %>%
  mutate(Coastal = FALSE)
NROcoast <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_NRO_Coast.xlsx',
                       sheet = 'JRH_Final_NRO_Coast') %>%
  mutate(Coastal = TRUE)
PRO <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_PRO.xlsx', sheet = 'JRH_Final_PRO') %>%
  mutate(Coastal = FALSE)
PROcoast <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_PRO_Coast.xlsx',
                       sheet = 'JRH_Final_Comment_Coast') %>%
  mutate(Coastal = TRUE)
TROcoast <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_TRO_Coast.xlsx',
                       sheet = 'JRH_Final_TRO_Coast2') %>%
  mutate(Coastal = TRUE)

# combine and just keep coastal vs not coastal designation
stations1 <- bind_rows(SCRO, SWRO, VRO, WCRO, NRO, NROcoast, PRO, PROcoast, TROcoast) %>%
  mutate(StationID = toupper(UID)) %>% # reserve UID for a concatenation of StationID and sample date
  distinct(StationID, .keep_all = T) %>% 
  dplyr::select(-UID)

# join coastal info
stations <- read_excel('data/regionalRefStressStationList/BioSitesAll.xlsx', 'UniqueSitesAllReff') %>% 
  mutate(StationID = toupper(UID)) %>% # reserve UID for a concatenation of StationID and sample date
  left_join(dplyr::select(stations1, StationID, Coastal), by ='StationID')%>% 
  dplyr::select(-UID) %>% 
  mutate(StationID = case_when(StationID == '5AXNOTC000.04' ~ '5AXNOTc000.04',
                               TRUE ~ as.character(StationID))) # rename dumb format

#clean up workspace
rm(SCRO); rm(SWRO); rm(VRO); rm(WCRO); rm(NRO); rm(NROcoast); rm(PRO); rm(PROcoast); rm(TROcoast); rm(stations1)
```


## Organize station information

Based on Lucy's initial spreadsheet with some common station descriptors, we are going to use what data is already available on the R server (compiled weekly for use in many other projects) and only create data where necessary.

Bring in what Lucy wants for analysis and pinned station data to identify overlaps.

```{r lucys spreadsheet}
lucy <- read.csv('data/Lucy/environmentaldataset.csv')
stationSpatialPin <- pin_get('ejones/WQM-Stations-Spatial', board = 'rsconnect')
stationFullPin <- pin_get('ejones/WQM-Station-Full', board = 'rsconnect') %>% 
  dplyr::select(STATION_ID, BASINS_HUC_12, WQS_ID, WQS_CLASS, WQS_SPSTDS, WQS_PWS, WQS_TROUT, WQS_TIER_III) %>% 
  distinct(STATION_ID, .keep_all = T) # there is one row per sample year in this dataset, so clean it up first
```

Join these data and identify where information is missing.

```{r initial station join}
stationsSpatial <- left_join(stations, stationSpatialPin, by = 'StationID') %>% 
  left_join(stationFullPin, by = c('StationID' = 'STATION_ID'))
```

make sure one site 
```{r}
# write.csv(
# stationsSpatial %>% 
#   group_by(StationID) %>% 
#   mutate(n = n()) %>% 
#   dplyr::select(n, everything()) %>% 
#   arrange(desc(n)), 
# 'JasonLookAtMe.csv')
```




We also need 1:100k strahler order for these stations, but that information is best not grabbed from the stationFullPin (originally from WQM Stations Full dataset on GIS REST service) as the exact process for identifying strahler order is not known.

This is best grabbed from most recent freshwater probmon wadeable dataset (IR2020) and where data are missing from that look for it in the last statewide BCG tolerance value data organization project (also contains Fairfax, MD, WVA, MAIA, MAHA, EPA, etc. data). There can be multiple rows per StationID in these datasets as each sample year is represented.

```{r 2020 freshwater probmon wadeable dataset}
prob <- read.csv('data/otherProjectData/Wadeable_ProbMon_2001-2018_Final_Final.csv') %>% 
  mutate(UID = paste(StationID, Year, sep = '_')) %>% 
  dplyr::select(StationID, UID, Order, totalArea_sqMile, ELEVMIN:siteRain_inyr) %>% 
  filter(StationID %in% stationsSpatial$StationID) 

BCG <- read.csv('data/otherProjectData/finalEnvData.csv') %>% 
  dplyr::select(StationID, UID, Order, totalArea_sqMile, ELEVMIN:siteRain_inyr) %>% 
  filter(StationID %in% filter(stationsSpatial, ! StationID %in% prob$StationID)$StationID)

#View(filter(stationsSpatial, ! StationID %in% prob$StationID))

stationsSpatial_withAllData <- left_join(stationsSpatial, bind_rows(prob, BCG), by = 'StationID') %>% 
  dplyr::select(JRH_Final_Ref_Cod:StationID, UID, everything())
```

We can try to get Order information from EDAS station information. This data is generated by the Biologists, so there is not consistency on what scale NHD the information comes from, but it is a start.

```{r EDAS}
EDASorder <- read_excel('data/otherProjectData/EDAScombinedStations.xlsx') %>% 
  dplyr::select(StationID, EDASOrder = Order)

stationsSpatial_withAllData <- left_join(stationsSpatial_withAllData, EDASorder, by = 'StationID') %>% 
  dplyr::select(JRH_Final_Ref_Cod:Order, EDASOrder, everything())
```



Calculate sum of missing data per field. For WQS information, only pay attention to WQS_ID field as other fields are okay to not have info.
```{r}
map_df(stationsSpatial_withAllData, function(x) sum(is.na(x)))
```

Missing lat/lng info from CEDS?
```{r}
missing <- filter(stationsSpatial_withAllData, is.na(Latitude))
```

Missing watershed, Order, WQS info

```{r}
needWQS <- filter(stationsSpatial_withAllData, is.na(WQS_ID))
needWatershed <- filter(stationsSpatial_withAllData, is.na(totalArea_sqMile))
saveRDS(needWQS, 'data/needSpatial/needWQS.RDS')
saveRDS(needWatershed, 'data/needSpatial/needWatershed.RDS')
```


Stations missing basic station info like lat/lng is alarming and needs to be investgated further? do these stations exist in WQM_Stations? Do they have benthic data in CEDS?


## Organize benthic information

For this, we will pull in pinned data from the server and transform it to a wide format to match project needs.

```{r pinned benthic data}
benSamps <- pin_get('ejones/benSamps', board = 'rsconnect') %>% 
  filter(StationID %in% stationsSpatial_withAllData$StationID) %>% 
  filter(`Target Count` == 200 & RepNum == 1) %>% # drop rarified samples and only rep 1's for now
  group_by(StationID) %>% 
  arrange(desc(`Collection Date`)) %>% 
  mutate(SampleN = 1:n()) %>%  # number samples from most recent collection
  filter(SampleN <= 4) # only keep most recent four samples

benthics <- pin_get('ejones/benthics', board = 'rsconnect') %>% 
  filter(BenSampID %in% benSamps$BenSampID) %>% 
  left_join(dplyr::select(benSamps, StationID, BenSampID, `Collection Date`, RepNum),
            by = c('BenSampID', 'StationID', 'RepNum')) %>% 
  group_by(StationID, BenSampID, RepNum) %>% 
  arrange(FinalID) %>%
  pivot_wider(id_cols = c('StationID','BenSampID','Collection Date', 'RepNum'), 
              names_from = FinalID, values_from = Individuals)%>%
  mutate(`Collection DateTime` = `Collection Date`,
         `Collection Date` = as.Date(`Collection Date`)) %>% 
  dplyr::select(StationID:`Collection Date`, `Collection DateTime`, everything()) %>% 
  arrange(`Collection Date`) 

```







```{r}
write_csv(stationsSpatial_withAllData, 'data/dataOut/stationsSpatial_withAllData.csv', na="")
write_csv(benthics, 'data/dataOut/benthics.csv', na="")

```



```{r}

FPissues <- pool %>% tbl(in_schema("wqm",  "Wqm_Field_Data_View")) %>%
  filter(Fdt_Spg_Code == 'FP' & Fdt_Depth < 0.3) %>% 
  as_tibble() 
```

