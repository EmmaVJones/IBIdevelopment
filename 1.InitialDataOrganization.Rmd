---
title: "Initial Data Organization"
author: "Emma Jones"
date: "6/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(sf)
library(DBI)
library(lubridate)
library(pool)
library(pins)
library(config)
library(readxl)
library(dbplyr)

# get ODS and server configuration settings
conn <- config::get("connectionSettings")

# connect to server
board_register_rsconnect(key = conn$CONNECT_API_KEY,  #Sys.getenv("CONNECT_API_KEY"),
                         server = conn$CONNECT_SERVER)#Sys.getenv("CONNECT_SERVER"))



# Connect to ODS production environment
# pool <- dbPool(
#   drv = odbc::odbc(),
#   Driver = "ODBC Driver 11 for SQL Server",#Driver = "SQL Server Native Client 11.0",
#   Server= "DEQ-SQLODS-PROD,50000",
#   dbname = "ODS",
#   trusted_connection = "yes")

```

## Bring in Stations with Reference/Stress qualifier

These spreadsheets are organized by DEQ region (one spreadsheet per regional office) where the regional biologists rated potential sites as reference or stressed. Jason Hill QAed the dataset and passed stations along to Emma Jones to query data for Lucy Baker to run NMDS and further exploratory analyses.

As more regional station lists become available, this chunk will fill out with all stations statewide.

```{r regional ref stress station lists}
# get old coastal designation
NROcoast <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_NRO_Coast.xlsx',
                       sheet = 'JRH_Final_NRO_Coast') %>%
  mutate(Coastal = TRUE)
PROcoast <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_PRO_Coast.xlsx',
                       sheet = 'JRH_Final_Comment_Coast') %>%
  mutate(Coastal = TRUE)
TROcoast <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_TRO_Coast.xlsx',
                       sheet = 'JRH_Final_TRO_Coast2') %>%
  mutate(Coastal = TRUE)

# combine and just keep coastal vs not coastal designation
stationsCoast <- bind_rows(NROcoast, PROcoast, TROcoast) %>%
  mutate(StationID = toupper(UID)) %>% # reserve UID for a concatenation of StationID and sample date
  distinct(StationID, .keep_all = T) %>% 
  dplyr::select(Coastal, StationID)

# join coastal info

stations <- read_excel('data/regionalRefStressStationList/FinalRefSites_AllSites.xlsx', 'FinalSitesAllv3') %>%
  group_by(UID) %>% 
  mutate(n = n()) %>% 
  filter(n > 1) %>% 
  arrange(UID, desc(n)) %>% 
  dplyr::select(n, everything())
write.csv(stations, 'stationHelp.csv', row.names = F)

stations <- read_excel('data/regionalRefStressStationList/FinalRefSites_AllSites.xlsx', 'FinalSitesAllv3') %>% 
  mutate(StationID = toupper(UID)) %>% # reserve UID for a concatenation of StationID and sample date
  left_join(dplyr::select(stationsCoast, StationID, Coastal), by ='StationID')%>% 
  dplyr::select(-UID) %>% 
  mutate(StationID = case_when(StationID == '5AXNOTC000.04' ~ '5AXNOTc000.04',
                               TRUE ~ as.character(StationID)))  # rename dumb format


#clean up workspace
rm(NROcoast); rm(PROcoast); rm(TROcoast); rm(stationsCoast)


```

Older method, included for reference. 
```{r regional ref stress station lists, eval=FALSE}
# SCRO <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_SCRO.xlsx', sheet = 'JRH_Final_SCRO') %>%
#   mutate(Coastal = FALSE)
# SWRO <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_SWRO.xlsx', sheet = 'JRH_Final_SWRO') %>%
#   mutate(Coastal = FALSE)
# VRO <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_VRO.xlsx', sheet = 'JRH_Final_VRO') %>%
#   mutate(Coastal = FALSE)
# WCRO <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_WCRO.xlsx', sheet = 'JRH_Final_WCRO') %>%
#   mutate(Coastal = FALSE)
# NRO <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_NRO.xlsx', sheet = 'JRH_Final_NRO') %>%
#   mutate(Coastal = FALSE)
# NROcoast <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_NRO_Coast.xlsx',
#                        sheet = 'JRH_Final_NRO_Coast') %>%
#   mutate(Coastal = TRUE)
# PRO <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_PRO.xlsx', sheet = 'JRH_Final_PRO') %>%
#   mutate(Coastal = FALSE)
# PROcoast <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_PRO_Coast.xlsx',
#                        sheet = 'JRH_Final_Comment_Coast') %>%
#   mutate(Coastal = TRUE)
# TROcoast <- read_excel('data/regionalRefStressStationList/RefStressDataFinal_TRO_Coast.xlsx',
#                        sheet = 'JRH_Final_TRO_Coast2') %>%
#   mutate(Coastal = TRUE)
# 
# # combine and just keep coastal vs not coastal designation
# stations1 <- bind_rows(SCRO, SWRO, VRO, WCRO, NRO, NROcoast, PRO, PROcoast, TROcoast) %>%
#   mutate(StationID = toupper(UID)) %>% # reserve UID for a concatenation of StationID and sample date
#   distinct(StationID, .keep_all = T) %>% 
#   dplyr::select(-UID)
# 
# # join coastal info
# stations <- read_excel('data/regionalRefStressStationList/BioSitesAll.xlsx', 'UniqueSitesAllReff') %>% 
#   mutate(StationID = toupper(UID)) %>% # reserve UID for a concatenation of StationID and sample date
#   left_join(dplyr::select(stations1, StationID, Coastal), by ='StationID')%>% 
#   dplyr::select(-UID) %>% 
#   mutate(StationID = case_when(StationID == '5AXNOTC000.04' ~ '5AXNOTc000.04',
#                                TRUE ~ as.character(StationID)))  # rename dumb format
# 
# 
# #clean up workspace
# rm(SCRO); rm(SWRO); rm(VRO); rm(WCRO); rm(NRO); rm(NROcoast); rm(PRO); rm(PROcoast); rm(TROcoast); rm(stations1)
```

## Organize benthic information 

For this, we will pull in pinned data from the server and transform it to a wide format to match project needs.

```{r pinned benthic data}
benSamps <- pin_get('ejones/benSamps', board = 'rsconnect') %>% 
  filter(StationID %in% stations$StationID) %>% 
  filter(`Target Count` == 200 & RepNum == 1) %>% # drop rarified samples and only rep 1's for now
  filter(Gradient != 'Boatable') %>% # drop any boatable methods collected at desired sites
  # first group by StationID and Date to drop duplicate Rep1 samples
  group_by(StationID, `Collection Date`) %>% 
  mutate(SampleN = 1:n()) %>% 
  # look at samples with multiple rep1's
  # dplyr::select(SampleN, everything()) %>%  
  # arrange(desc(SampleN))
  # write.csv(benSamps1, 'multiple rep 1s.csv', row.names = F)
  filter(! SampleN > 1) %>%  # drop multiple rep 1s before filtering on max 4 samples per site
  ungroup() %>% 
  
  # then drop multiple reps of the same season (not taken on the same day)
  mutate(Year = year(`Collection Date`)) %>% 
  group_by(StationID, Year, Season) %>% 
  mutate(SampleN = 1:n()) %>% 
  # look at samples with multiple rep1's in a season
  # dplyr::select(SampleN, Year, Season, everything()) %>% 
  # arrange(desc(SampleN)) %>% 
  filter(! SampleN > 1) %>% 
  
  # then total all samples taken and drop any records > 4 for a station, starting the count at the most recent sample event
  ungroup() %>% 
  group_by(StationID) %>% 
  arrange(desc(`Collection Date`)) %>% 
  mutate(`Collection Date` = as.Date(`Collection Date`),
         SampleN = 1:n(),# number samples from most recent collection
         JulianDate = yday(`Collection Date`)) %>%  
  filter(SampleN <= 4)  # only keep most recent four samples


benthics <- pin_get('ejones/benthics', board = 'rsconnect') %>% 
  filter(BenSampID %in% benSamps$BenSampID) %>% 
  left_join(dplyr::select(benSamps, StationID, BenSampID, `Collection Date`, RepNum),
            by = c('BenSampID', 'StationID', 'RepNum')) %>% 
  group_by(StationID, BenSampID, RepNum) %>% 
  arrange(FinalID) %>%
  pivot_wider(id_cols = c('StationID','BenSampID','Collection Date', 'RepNum'), 
              names_from = FinalID, values_from = Individuals)%>%
  mutate(`Collection DateTime` = `Collection Date`,
         `Collection Date` = as.Date(`Collection Date`)) %>% 
  dplyr::select(StationID:`Collection Date`, `Collection DateTime`, everything()) %>% 
  arrange(`Collection Date`) 

```

## Organize station information to join back to benthic info

Based on Lucy's initial spreadsheet with some common station descriptors, we are going to use what data is already available on the R server (compiled weekly for use in many other projects) and only create data where necessary.

Bring in what Lucy wants for analysis and pinned station data to identify overlaps.

```{r lucys spreadsheet}
lucy <- read.csv('data/Lucy/environmentaldataset.csv')
stationSpatialPin <- pin_get('ejones/WQM-Stations-Spatial', board = 'rsconnect')
stationFullPin <- pin_get('ejones/WQM-Station-Full', board = 'rsconnect') %>% 
  dplyr::select(STATION_ID, BASINS_HUC_12, WQS_ID, WQS_CLASS, WQS_SPSTDS, WQS_PWS, WQS_TROUT, WQS_TIER_III) %>% 
  distinct(STATION_ID, .keep_all = T) # there is one row per sample year in this dataset, so clean it up first
```

Join these data and identify where information is missing.

```{r initial station join}
stationsSpatial <- left_join(benSamps, stations, by = 'StationID') %>% 
  left_join(stationSpatialPin, by = 'StationID') %>% 
  left_join(stationFullPin, by = c('StationID' = 'STATION_ID'))

# fix WQS info if the stationFullPin doesn't have info
if(nrow(filter(stationsSpatial, is.na(WQS_ID))) > 0){
  WQSlookup_withStandards <- pin_get('ejones/WQSlookup-withStandards', board = 'rsconnect')
  
  fixMe <- filter(stationsSpatial, is.na(WQS_ID)) %>% 
    dplyr::select(-c(WQS_ID:WQS_TIER_III)) %>% 
    left_join(
      # rename to pin names
      dplyr::select(WQSlookup_withStandards, StationID, WQS_ID, WQS_CLASS = CLASS, WQS_SPSTDS = SPSTDS, 
                    WQS_PWS = PWS, WQS_TROUT = Trout, WQS_TIER_III = Tier_III), by = 'StationID')
  stationsSpatial <- bind_rows(filter(stationsSpatial, ! is.na(WQS_ID)),
                               fixMe)
  rm(fixMe); rm(WQSlookup_withStandards)
}

```

make sure one site 
```{r}
# write.csv(
# stationsSpatial %>% 
#   group_by(StationID) %>% 
#   mutate(n = n()) %>% 
#   dplyr::select(n, everything()) %>% 
#   arrange(desc(n)), 
# 'JasonLookAtMe.csv')
```




We also need 1:100k strahler order for these stations, but that information is best not grabbed from the stationFullPin (originally from WQM Stations Full dataset on GIS REST service) as the exact process for identifying strahler order is not known.

This is best grabbed from most recent freshwater probmon wadeable dataset (IR2020) and where data are missing from that look for it in the last statewide BCG tolerance value data organization project (also contains Fairfax, MD, WVA, MAIA, MAHA, EPA, etc. data). There can be multiple rows per StationID in these datasets as each sample year is represented.

```{r 2020 freshwater probmon wadeable dataset}
prob <- read.csv('data/otherProjectData/Wadeable_ProbMon_2001-2018_Final_Final.csv') %>% 
  mutate(UID = paste(StationID, Year, sep = '_')) %>% 
  dplyr::select(StationID, UID, Year, Order, totalArea_sqMile, ELEVMIN:siteRain_inyr) %>% 
  filter(StationID %in% stationsSpatial$StationID) 

BCG <- read.csv('data/otherProjectData/finalEnvData.csv') %>% 
  dplyr::select(StationID, UID, Year, Order, totalArea_sqMile, ELEVMIN:siteRain_inyr) %>% 
  filter(StationID %in% filter(stationsSpatial, ! StationID %in% prob$StationID)$StationID)

#View(filter(stationsSpatial, ! StationID %in% prob$StationID))

stationsSpatial2 <- left_join(stationsSpatial, bind_rows(prob, BCG), by = c('StationID', 'Year')) %>% 
  dplyr::select(DataSource, JRH_Final_Ref_Cod:Coastal, StationID, UID, Year, Season, JulianDate, `Collection Date`, Sta_Desc,
                Latitude, Longitude, US_L3CODE:wshedRain_inyr, BenSampID, RepNum, Gradient, `Target Count`)

# stationsSpatial2 <- left_join(stationsSpatial, bind_rows(prob, BCG), by = c('StationID', 'Year')) %>% 
#   dplyr::select(JRH_Final_Ref_Cod:StationID, UID, Year,  everything())

# Still all years in dataset that were sampled. These need to be dropped once we get benthic info joined
# View(
# stationsSpatial2 %>%
#   group_by(StationID) %>%
#   mutate(n = n()) %>%
#   dplyr::select(n, everything()) %>%
#   arrange(desc(n), StationID, desc(Year))
# )
```

We can try to get Order information from EDAS station information. This data is generated by the Biologists, so there is not consistency on what scale NHD the information comes from, but it is a start.

```{r EDAS}
EDASorder <- read_excel('data/otherProjectData/EDAScombinedStations.xlsx') %>% 
  dplyr::select(StationID, EDASOrder = Order)

stationsSpatial2 <- left_join(stationsSpatial2, EDASorder, by = 'StationID') %>% 
  dplyr::select(DataSource:Order, EDASOrder, everything()) %>% 
  mutate( UID = paste(StationID, year(`Collection Date`), Season, sep= '_'))
#dplyr::select(JRH_Final_Ref_Cod:Order, EDASOrder, everything())

rm(EDASorder); rm(BCG); rm(prob); rm(stationSpatialPin); rm(stationFullPin)
```



Calculate sum of missing data per field. For WQS information, only pay attention to WQS_ID field as other fields are okay to not have info.
```{r}
map_df(stationsSpatial2, function(x) sum(is.na(x)))
```

Missing lat/lng info from CEDS?
```{r}
#missing <- filter(stationsSpatial2, is.na(Latitude))
```

Missing watershed, Order, WQS info

```{r}
needWQS <- filter(stationsSpatial_withAllData, is.na(WQS_ID)) %>% 
  distinct(StationID, .keep_all = T)
needWatershed <- filter(stationsSpatial2, is.na(totalArea_sqMile)) %>% 
   distinct(StationID, .keep_all = T)
#saveRDS(needWQS, 'data/needSpatial/needWQS.RDS')
#saveRDS(needWatershed, 'data/needSpatial/needWatershed.RDS')
```



### Update: After manual review, bring in final datasets

only run once

```{r WQS addition}
# WQS <- readRDS('data/needSpatial/finalWQS.RDS') %>% 
#   filter(! is.na(WQS_ID)) %>% 
#   bind_rows(
#     tibble(StationID = c('4AJHN000.01', '5BXAT000.30', '7-RTT000.74', '2-GDC000.52', '2-BSB000.20',
#                          '2-CPN004.81', '7-TOM001.73'),
#        WQS_ID = c('RL_4A_109354', 'EL_5B_030029', 'EL_7C_050117', 'RL_2B_175050', 'RL_2A_076264',
#                   'RL_2C_268362', 'RL_7C_298739'),
#        `Buffer Distance` = c('No connections within 80 m', 'No connections within 80 m',
#                              'No connections within 80 m','No connections within 80 m', 
#                              'No connections within 80 m','No connections within 80 m',
#                              'No connections within 80 m'),
#        Comments = c('IBI project review', 'IBI project review', 'IBI project review', 'IBI project review',
#                     'IBI project review', 'IBI project review', 'IBI project review' )) )
# 
# stationsSpatial3 <- stationsSpatial2 %>% 
#   left_join(dplyr::select(WQS, StationID, WQS_IDnew = WQS_ID), by = 'StationID') %>% 
#   mutate(WQS_ID = case_when(is.na(WQS_ID) ~ as.character(WQS_IDnew),
#                             TRUE~ as.character(WQS_ID))) %>% 
#   filter(is.na(WQS_ID))
#
# # add these to the official WQSlookup and WQSlookup-withStandards pins
# 
# WQSlookup <- pin_get('ejones/WQSlookup', board = 'rsconnect')
# WQSlookup_withStandards <- pin_get('ejones/WQSlookup-withStandards', board = 'rsconnect')
# 
# WQSlookup <- bind_rows(WQSlookup, WQS)
# 
# WQSlookupToDo <- filter(WQSlookup, StationID %in% WQS$StationID)
# 
# #bring in Riverine layers, valid for the assessment window
# riverine <- st_read('C:/HardDriveBackup/GIS/WQS/WQS_layers_05082020.gdb', layer = 'riverine_05082020') %>%
#   st_drop_geometry() # only care about data not geometry
# 
# WQSlookupFull <- left_join(WQSlookupToDo, riverine) %>%
#   filter(!is.na(CLASS))
# rm(riverine)
# 
# estuarineLines <- st_read('C:/HardDriveBackup/GIS/WQS/WQS_layers_05082020.gdb', layer = 'estuarinelines_05082020') %>%
#   st_drop_geometry() # only care about data not geometry
# 
# WQSlookupFull <- left_join(WQSlookupToDo, estuarineLines) %>%
#   filter(!is.na(CLASS)) %>%
#   bind_rows(WQSlookupFull)
# rm(estuarineLines)
# 
# WQSlookup_withStandards <- bind_rows(WQSlookup_withStandards, WQSlookupFull) # for pin
# 
# # pin back to server
# pin(WQSlookup_withStandards, description = "WQS lookup table with Standards from metadata attribution application", board = "rsconnect")
# pin(WQSlookup, description = "WQS lookup table from metadata attribution application", board = "rsconnect")
```


#### Bring in final Order info

```{r}
Order <- readRDS('data/needSpatial/finalOrder.RDS') %>% 
  bind_rows(
    tibble(StationID = c('2-CPN004.81','2-CRL001.83', '2-CRL004.04', '5ACHP002.03','7-GRS002.29', '7-PAR004.35','7-TOM001.73'),
           STRAHLER = c('3','1','1','2','1','1','1'))  )

fixOrder <- stationsSpatial2 %>% 
  filter(is.na(Order)) %>% 
  left_join(dplyr::select(Order, StationID, OrderNew = STRAHLER), by = 'StationID') %>% 
  mutate(Order = case_when(is.na(Order) ~ as.numeric(OrderNew), 
                           TRUE ~ as.numeric(Order))) %>% 
  dplyr::select(-OrderNew)

stationsSpatial3 <- filter(stationsSpatial2, !is.na(Order)) %>% 
  bind_rows(fixOrder)

```



Double check we have everything populated that we need for NMDS

```{r}
map_df(stationsSpatial3, function(x) sum(is.na(x)))
```




We need to send the nmds all categorical and continuous info in a single dataset, so join the date, gradient, etc. from benSamps to the station info for analysis.

```{r}
stationsSpatial_withAllData <- stationsSpatial3 %>% arrange(StationID, `Collection Date`)
# stationsSpatial_withAllData <- left_join(
#   dplyr::select(benSamps, StationID, `Collection Date`, Year, Gradient, Season, JulianDate),
#   stationsSpatial2, by = c('StationID', 'Year')) %>% 
#   mutate(UID = paste(StationID, year(`Collection Date`), Season, sep= '_')) %>% 
#   dplyr::select(DataSource, JRH_Final_Ref_Cod, StationID, UID, everything()) %>% 
#   arrange(StationID, `Collection Date`)

# make sure no one has > 4 samples
View(stationsSpatial_withAllData %>%
  mutate(SampleN = 1:n()) %>%
  dplyr::select(SampleN, everything()) %>%
  arrange(desc(SampleN)) %>% 
    filter(SampleN > 4)
)
```






```{r}
write_csv(stationsSpatial_withAllData, 'data/dataOut/stationsSpatial_withAllData.csv', na="")
write_csv(benthics, 'data/dataOut/benthics.csv', na="")

```


